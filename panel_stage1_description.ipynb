{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pre-processing page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pre-processing page, I have included several tabs that correspond to various pre-processing steps.  \n",
    "\n",
    "    1. Tokenization\n",
    "    2. Removal of stop words\n",
    "    3. Stemming words\n",
    "    4. Work embedding\n",
    "    \n",
    "\n",
    "When you click \"continue\" all the options that were set on each tab will be used to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to realize when creating a panel pipeline is that each page must be built out of a paramaterized class.  It will need to include a `panel` method, which specifies how the pane should display.  It should also include an `output` function with the `param.output` decorator with variables to be passed to the next stage.  Here I am going to create my first stage by making a class called `PreProcessor`.  I will make sure to include the output and panel functions and will fill in with other needed parts to make my stage.\n",
    "\n",
    "I am going to quickly go through my process to build this first stage, but will not explain everything in detail.  Please see panel docs for more information on setting up simple panel app.\n",
    "\n",
    "Here is the barebones start of my `PreProcessor` class.  I have included an `output` and a `panel` function.  We will fill these in as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class PreProcessor(param.Parameterized):\n",
    "    \n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        \n",
    "        \n",
    "    @param.output()\n",
    "    def output(self):\n",
    "        return #output\n",
    "    \n",
    "   \n",
    "    \n",
    "    def panel(self):\n",
    "        \n",
    "        return #panel set up\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to begin filling in with more pieces by building the tabs.  First, I want to set the parts in place to display these tabs.  There is also a continue button included on the page, so I will put this just below the tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class PreProcessor(param.Parameterized):\n",
    "    \n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        \n",
    "        self.continue_button = pn.widgets.Button(name='Continue',\n",
    "                                                 width = 100,\n",
    "                                                 button_type='primary')\n",
    "        \n",
    "    @param.output()\n",
    "    def output(self):\n",
    "        return #output\n",
    "\n",
    "    \n",
    "    def panel(self):\n",
    "        \n",
    "        return return pn.Column(\n",
    "            pn.Tabs(\n",
    "                    #tabs to be added.\n",
    "                ),\n",
    "            self.continue_button\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I am ready to accept tabs, let's create a tab to include on the page.  I decided that each tab would have roughly the same layout.  It would include a left pane and right pane.  The left pane would include the name of the pre-processing step, some explanatory text, then all the widgets needed to insert information for this step (buttons, checkboxes, input text, etc).  The right side will be a display pane for the uploaded text, and eventually the final results.  \n",
    "\n",
    "I'll start by including the first tab  that loads text.  I'll simply create another function called `load_text` that can be inserted into the `tabs` portion of the `panel` function.  The `load_text_page` function is basically like a `panel` function in that it will return a panel layout.  `load_text_page` includes a file input button that will need to be uploaded properly, which will be done with `load_df`.  \n",
    "\n",
    "Furthermore, the right side will include a display of the dataframe once loaded.  I have created a function, `df_pane`, that will display this dataframe, `display_df`, when it is present.  `df_pane` will be used on any page you choose to display a dataframe.  Currently, I have it set to display only on the load text tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class PreProcessor(param.Parameterized):\n",
    "    \n",
    "    df = param.DataFrame()\n",
    "    \n",
    "    display_df = param.DataFrame(default = pd.DataFrame())\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        \n",
    "        # button for the pre-processing page\n",
    "        self.continue_button = pn.widgets.Button(name='Continue',\n",
    "                                                 width = 100,\n",
    "                                                 button_type='primary')\n",
    "        \n",
    "        # load text widgets \n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        self.load_file = pn.widgets.FileInput()\n",
    "        self.load_file.link(self.df, callbacks={'value': self.load_df})\n",
    "\n",
    "        \n",
    "    @param.output()\n",
    "    def output(self):\n",
    "        return #output\n",
    "    \n",
    "    \n",
    "    @param.depends('display_df')\n",
    "    def df_pane(self):\n",
    "        return pn.WidgetBox(self.display_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "    \n",
    "    \n",
    "    # load text page functions\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def load_df(self, df, event):\n",
    "        info = io.BytesIO(self.load_file.value)\n",
    "        if self.header_checkbox.value==True:\n",
    "            self.df = pd.read_csv(info)\n",
    "        else:\n",
    "            self.df = pd.read_csv(info, sep='\\n', header = None, names=['text'])\n",
    "        \n",
    "        self.display_df = self.df\n",
    "    \n",
    "    def load_text_page(self):\n",
    "        helper_text = (\n",
    "            \"This simple Sentiment Analysis NLP app will allow you to select a few different options \" +\n",
    "            \"for some preprocessing steps to prepare your text for testing and training. \" +\n",
    "            \"It will then allow you to choose a model to train, the percentage of data to \" +\n",
    "            \"preserve for test, while the rest will be used to train the model.  Finally, \" +\n",
    "            \"some initial metrics will be displayed to determine how well the model did to predict \" +\n",
    "            \"the testing results.\" +\n",
    "            \" \" +\n",
    "            \"Please choose a csv file that contains lines of text to analyze.  This text should \" +\n",
    "            \"have a text column as well as a sentiment column.  If there is a header included in the file, \" +\n",
    "            \"make sure to check the header checkbox.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Text:'),\n",
    "                    pn.Column(\n",
    "                        helper_text,\n",
    "                         self.header_checkbox,\n",
    "                         self.load_file\n",
    "                        ),\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    self.df_pane,\n",
    "                    \n",
    "                )\n",
    "        \n",
    "        )\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    def panel(self):\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Tabs(\n",
    "                ('Load Text', self.load_text_page),\n",
    "                ),\n",
    "            self.continue_button\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can run the following to see how our panel app is coming together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessor()\n",
    "pp.panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one tab in place; now, I will go ahead and add the other functions, widgets and signals needed for each of the other tabs.  They will all be done in a similar fashion to the load text tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "import pandas as pd\n",
    "\n",
    "class PreProcessor(param.Parameterized):\n",
    "    \n",
    "    #df will be the variable holding the dataframe of text\n",
    "    df = param.DataFrame()\n",
    "    #title to display for each tab\n",
    "    name_of_page = param.String(default = 'Name of page')\n",
    "    #dataframe to display.\n",
    "    display_df = param.DataFrame(default = pd.DataFrame())\n",
    "    # stopword_df is the dataframe containing the stopewords\n",
    "    stopword_df = param.DataFrame(default = pd.DataFrame())\n",
    "    \n",
    "    stopwords = param.List(default = [])\n",
    "    X = param.Array(default = None)\n",
    "    \n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # button for the pre-processing page\n",
    "        self.continue_button = pn.widgets.Button(name='Continue',\n",
    "                                                 width = 100,\n",
    "                                                 button_type='primary')\n",
    "        \n",
    "        # load text widgets \n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        self.load_file = pn.widgets.FileInput()\n",
    "        self.load_file.link(self.df, callbacks={'value': self.load_df})\n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        \n",
    "        # tokenize widgets\n",
    "        self.search_pattern_input = pn.widgets.TextInput(name='Search Pattern', value = '\\w+', width = 100)\n",
    "        \n",
    "        # remove stop words widgets\n",
    "        self.load_words_button = pn.widgets.FileInput()\n",
    "        self.load_words_button.link(self.stopwords, callbacks={'value': self.load_stopwords})\n",
    "        \n",
    "        # stem widgets\n",
    "        self.stem_choice = pn.widgets.Select(name='Select', options=['Porter', 'Snowball'])\n",
    "        \n",
    "        # embedding widgets\n",
    "        \n",
    "        self.we_model = pn.widgets.Select(name='Select', options=['SKLearn Count Vectorizer'])\n",
    "\n",
    "        \n",
    "    @param.output()\n",
    "    def output(self):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    @param.depends('display_df')\n",
    "    def df_pane(self):\n",
    "        return pn.WidgetBox(self.display_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "    \n",
    "    # load text page functions\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def load_df(self, df, event):\n",
    "        info = io.BytesIO(self.load_file.value)\n",
    "        if self.header_checkbox.value==True:\n",
    "            self.df = pd.read_csv(info)\n",
    "        else:\n",
    "            self.df = pd.read_csv(info, sep='\\n', header = None, names=['text'])\n",
    "        \n",
    "        self.display_df = self.df\n",
    "    \n",
    "    def load_text_page(self):\n",
    "        helper_text = (\n",
    "            \"This simple Sentiment Analysis NLP app will allow you to select a few different options \" +\n",
    "            \"for some preprocessing steps to prepare your text for testing and training. \" +\n",
    "            \"It will then allow you to choose a model to train, the percentage of data to \" +\n",
    "            \"preserve for test, while the rest will be used to train the model.  Finally, \" +\n",
    "            \"some initial metrics will be displayed to determine how well the model did to predict \" +\n",
    "            \"the testing results.\" +\n",
    "            \" \" +\n",
    "            \"Please choose a csv file that contains lines of text to analyze.  This text should \" +\n",
    "            \"have a text column as well as a sentiment column.  If there is a header included in the file, \" +\n",
    "            \"make sure to check the header checkbox.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Text:'),\n",
    "                    pn.Column(\n",
    "                        helper_text,\n",
    "                         self.header_checkbox,\n",
    "                         self.load_file\n",
    "                        ),\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    self.df_pane,\n",
    "                    \n",
    "                )\n",
    "        \n",
    "        )\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # tokenize page options\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def tokenize_option_page(self):\n",
    "        \n",
    "        help_text = (\"Tokenization will break your text into a list of single articles \" +\n",
    "            \"(ex. ['A', 'cat', 'walked', 'into', 'the', 'house', '.']).  Specify a regular \" +\n",
    "            \"expression (regex) search pattern to use for splitting the text.\")\n",
    "        \n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Tokenize options:'),\n",
    "                    pn.WidgetBox(help_text, self.search_pattern_input,\n",
    "                                    height = 300,\n",
    "                                    width = 300\n",
    "        \n",
    "                                )\n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # remove stopwords page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def remove_stopwords_page(self):\n",
    "        \n",
    "        help_text = (\n",
    "            \"Stop words are words that do not add any value to the sentiment of the text. \" +\n",
    "            \"Removing them may improve your sentiment results.  You may load a list of stop words \" +\n",
    "            \"to exclude from your text.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Stopwords:'),\n",
    "                    pn.WidgetBox(help_text, self.load_words_button,\n",
    "                                    height = 300,\n",
    "                                    width = 300\n",
    "        \n",
    "                    )\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    pn.WidgetBox(self.stopword_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "                    \n",
    "                )\n",
    "        )\n",
    "    \n",
    "    def load_stopwords(self, stopwords, event):\n",
    "        info = io.BytesIO(self.load_words_button.value)\n",
    "        self.stopwords = pd.read_pickle(info)\n",
    "        self.stopword_df = pd.DataFrame({'stop words': self.stopwords})\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # stemming page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def stemmer_page(self):\n",
    "        help_text = (\n",
    "            \"Stemming is a normalization step for the words in your text.  Something that is \" +\n",
    "            \"plural should probably still be clumped together with a singular version of a word, \" +\n",
    "            \"for example.  Stemming will basically remove the ends of words.  Here you can choose \" + \n",
    "            \"between a Porter Stemmer or Snowball Stemmer. Porter is a little less aggressive than \" +\n",
    "            \"Snowball, however, Snowball is considered a slight improvement over Porter.\"\n",
    "        )\n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Stemmer options:'),\n",
    "                    pn.WidgetBox(help_text, self.stem_choice,\n",
    "                height = 300,\n",
    "                width = 300)\n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # embedding page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def word_embedding_page(self):\n",
    "        \n",
    "        help_text = (\"Embedding the process of turning words into numerical vectors. \" +\n",
    "                    \"There have been several algorithms developed to do this, however, currently in this \" +\n",
    "                    \"app, the sklearn count vectorizer is available. This algorithm will return a sparse \" +\n",
    "                    \"matrix represention of all the words in your text.\"\n",
    "                    )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Choose embedding model:'),\n",
    "                    pn.WidgetBox(help_text, self.we_model,\n",
    "                            height = 300,\n",
    "                            width = 300\n",
    "        \n",
    "                    )\n",
    "        \n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    def panel(self):\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Tabs(\n",
    "                ('Load Text', self.load_text_page),\n",
    "                ('Tokenize', self.tokenize_option_page),\n",
    "                ('Remove Stopwords', self.remove_stopwords_page),\n",
    "                ('Stem', self.stemmer_page),\n",
    "                ('Embed', self.word_embedding_page)\n",
    "                ),\n",
    "            self.continue_button\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our panel app by running the cell below. As you can see, it is starting to come together.  This is pretty much the first stage!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessor()\n",
    "pp.panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing we don't have is the actual processing being done on the data.  We can set up all our choices for each tab, but we haven't done anything with it yet.  This is where the `continue` button will come into play.  With the `continue` button, we can run our processing, and send a signal that its time to move on to the next page.  I will explain more about moving onto the next page in a moment. Let's first add a function that will put all of our pre-processing pieces together.  The `continue_ready` function will be added to our PreProcessor class, and the continue button will be told to run this function when it is clicked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "import pandas as pd\n",
    "\n",
    "class PreProcessor(param.Parameterized):\n",
    "    \n",
    "    #df will be the variable holding the dataframe of text\n",
    "    df = param.DataFrame()\n",
    "    #title to display for each tab\n",
    "    name_of_page = param.String(default = 'Name of page')\n",
    "    #dataframe to display.\n",
    "    display_df = param.DataFrame(default = pd.DataFrame())\n",
    "    # stopword_df is the dataframe containing the stopewords\n",
    "    stopword_df = param.DataFrame(default = pd.DataFrame())\n",
    "    \n",
    "    stopwords = param.List(default = [])\n",
    "    X = param.Array(default = None)\n",
    "    \n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # button for the pre-processing page\n",
    "        self.continue_button = pn.widgets.Button(name='Continue',\n",
    "                                                 width = 100,\n",
    "                                                 button_type='primary')\n",
    "        # ****NEW ADDITION******\n",
    "        self.continue_button.on_click(self.continue_ready)\n",
    "        # **********************\n",
    "        \n",
    "        # load text widgets \n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        self.load_file = pn.widgets.FileInput()\n",
    "        self.load_file.link(self.df, callbacks={'value': self.load_df})\n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        \n",
    "        # tokenize widgets\n",
    "        self.search_pattern_input = pn.widgets.TextInput(name='Search Pattern', value = '\\w+', width = 100)\n",
    "        \n",
    "        # remove stop words widgets\n",
    "        self.load_words_button = pn.widgets.FileInput()\n",
    "        self.load_words_button.link(self.stopwords, callbacks={'value': self.load_stopwords})\n",
    "        \n",
    "        # stem widgets\n",
    "        self.stem_choice = pn.widgets.Select(name='Select', options=['Porter', 'Snowball'])\n",
    "        \n",
    "        # embedding widgets\n",
    "        \n",
    "        self.we_model = pn.widgets.Select(name='Select', options=['SKLearn Count Vectorizer'])\n",
    "\n",
    "        \n",
    "    @param.output()\n",
    "    def output(self):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    @param.depends('display_df')\n",
    "    def df_pane(self):\n",
    "        return pn.WidgetBox(self.display_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "    \n",
    "    # load text page functions\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def load_df(self, df, event):\n",
    "        info = io.BytesIO(self.load_file.value)\n",
    "        if self.header_checkbox.value==True:\n",
    "            self.df = pd.read_csv(info)\n",
    "        else:\n",
    "            self.df = pd.read_csv(info, sep='\\n', header = None, names=['text'])\n",
    "        \n",
    "        self.display_df = self.df\n",
    "    \n",
    "    def load_text_page(self):\n",
    "        helper_text = (\n",
    "            \"This simple Sentiment Analysis NLP app will allow you to select a few different options \" +\n",
    "            \"for some preprocessing steps to prepare your text for testing and training. \" +\n",
    "            \"It will then allow you to choose a model to train, the percentage of data to \" +\n",
    "            \"preserve for test, while the rest will be used to train the model.  Finally, \" +\n",
    "            \"some initial metrics will be displayed to determine how well the model did to predict \" +\n",
    "            \"the testing results.\" +\n",
    "            \" \" +\n",
    "            \"Please choose a csv file that contains lines of text to analyze.  This text should \" +\n",
    "            \"have a text column as well as a sentiment column.  If there is a header included in the file, \" +\n",
    "            \"make sure to check the header checkbox.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Text:'),\n",
    "                    pn.Column(\n",
    "                        helper_text,\n",
    "                         self.header_checkbox,\n",
    "                         self.load_file\n",
    "                        ),\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    self.df_pane,\n",
    "                    \n",
    "                )\n",
    "        \n",
    "        )\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # tokenize page options\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def tokenize_option_page(self):\n",
    "        \n",
    "        help_text = (\"Tokenization will break your text into a list of single articles \" +\n",
    "            \"(ex. ['A', 'cat', 'walked', 'into', 'the', 'house', '.']).  Specify a regular \" +\n",
    "            \"expression (regex) search pattern to use for splitting the text.\")\n",
    "        \n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Tokenize options:'),\n",
    "                    pn.WidgetBox(help_text, self.search_pattern_input,\n",
    "                                    height = 300,\n",
    "                                    width = 300\n",
    "        \n",
    "                                )\n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # remove stopwords page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def remove_stopwords_page(self):\n",
    "        \n",
    "        help_text = (\n",
    "            \"Stop words are words that do not add any value to the sentiment of the text. \" +\n",
    "            \"Removing them may improve your sentiment results.  You may load a list of stop words \" +\n",
    "            \"to exclude from your text.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Stopwords:'),\n",
    "                    pn.WidgetBox(help_text, self.load_words_button,\n",
    "                                    height = 300,\n",
    "                                    width = 300\n",
    "        \n",
    "                    )\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    pn.WidgetBox(self.stopword_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "                    \n",
    "                )\n",
    "        )\n",
    "    \n",
    "    def load_stopwords(self, stopwords, event):\n",
    "        info = io.BytesIO(self.load_words_button.value)\n",
    "        self.stopwords = pd.read_pickle(info)\n",
    "        self.stopword_df = pd.DataFrame({'stop words': self.stopwords})\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # stemming page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def stemmer_page(self):\n",
    "        help_text = (\n",
    "            \"Stemming is a normalization step for the words in your text.  Something that is \" +\n",
    "            \"plural should probably still be clumped together with a singular version of a word, \" +\n",
    "            \"for example.  Stemming will basically remove the ends of words.  Here you can choose \" + \n",
    "            \"between a Porter Stemmer or Snowball Stemmer. Porter is a little less aggressive than \" +\n",
    "            \"Snowball, however, Snowball is considered a slight improvement over Porter.\"\n",
    "        )\n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Stemmer options:'),\n",
    "                    pn.WidgetBox(help_text, self.stem_choice,\n",
    "                height = 300,\n",
    "                width = 300)\n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # embedding page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def word_embedding_page(self):\n",
    "        \n",
    "        help_text = (\"Embedding the process of turning words into numerical vectors. \" +\n",
    "                    \"There have been several algorithms developed to do this, however, currently in this \" +\n",
    "                    \"app, the sklearn count vectorizer is available. This algorithm will return a sparse \" +\n",
    "                    \"matrix represention of all the words in your text.\"\n",
    "                    )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Choose embedding model:'),\n",
    "                    pn.WidgetBox(help_text, self.we_model,\n",
    "                            height = 300,\n",
    "                            width = 300\n",
    "        \n",
    "                    )\n",
    "        \n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # *****NEW ADDITION****************************************************************        \n",
    "    def continue_ready(self, event):\n",
    "\n",
    "        # Set up for tokenization\n",
    "        tokenizer = RegexpTokenizer(self.search_pattern_input.value)\n",
    "\n",
    "        # Set up for stemming\n",
    "        if self.stem_choice.value == 'Porter':\n",
    "            stemmer = PorterStemmer() \n",
    "        else:\n",
    "            stemmer = SnowballStemmer()\n",
    "\n",
    "        # Set up for embedding\n",
    "        if self.we_model.value == 'SKLearn Count Vectorizer':\n",
    "            # Create a vectorizer instance\n",
    "            vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "        corpus = []\n",
    "        #loop through each line of data\n",
    "        for n in range(len(self.display_df)):  \n",
    "            sentence = self.display_df.iloc[n].text\n",
    "\n",
    "            #1. Tokenize\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "            #2. remove stop words\n",
    "            tokens_no_sw = [word for word in tokens if not word in self.stopwords]\n",
    "\n",
    "            #3. stem the remaining words\n",
    "            stem_words = [stemmer.stem(x) for x in tokens_no_sw]\n",
    "\n",
    "            #Join the words back together as one string and append this string to your corpus.\n",
    "            corpus.append(' '.join(stem_words))\n",
    "\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        labels = self.display_df['sentiment']\n",
    "\n",
    "        xlist = []\n",
    "        for n in range(len(X)):\n",
    "            xlist.append(list(X[n]))\n",
    "        self.X = X\n",
    "        self.display_df = pd.DataFrame({'embeddings': xlist, 'sentiment': labels})\n",
    "        \n",
    "        #*********************************************************************************\n",
    "        \n",
    "    \n",
    "    def panel(self):\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Tabs(\n",
    "                ('Load Text', self.load_text_page),\n",
    "                ('Tokenize', self.tokenize_option_page),\n",
    "                ('Remove Stopwords', self.remove_stopwords_page),\n",
    "                ('Stem', self.stemmer_page),\n",
    "                ('Embed', self.word_embedding_page)\n",
    "                ),\n",
    "            self.continue_button\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the first page is done as a stand alone page.  We will have to add some more to it later to fit into our pipeline, but I want to first introduce our second page as there are some details to note regarding this stage.  I am not going to go into as much detail to build it out, but I will say that it is the same layout as the first page.  It will start off displaying a sample of the word embeddings dataframe, then, when the metrics from the train and testing are computed, the display will change to show these results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.base import base_page\n",
    "import panel as pn\n",
    "import pandas as pd\n",
    "import param\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class trainer(param.Parameterized):\n",
    "    \n",
    "    display_df = param.DataFrame(default = pd.DataFrame())\n",
    "    \n",
    "    results = param.Boolean(default = False)\n",
    "    \n",
    "    X = param.Array(default = None)\n",
    "    \n",
    "    result_string = param.String(default = '')\n",
    "\n",
    "    result_string = param.String('')\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        self.name_of_page = 'Test and Train'\n",
    "        \n",
    "        self.test_slider = pn.widgets.IntSlider(name='Test Percentage', start=0, end=100, step=10, value=20)\n",
    "\n",
    "        self.tt_button = pn.widgets.Button(name='Train and Test', button_type='primary')\n",
    "        self.tt_button.on_click(self.train_test)\n",
    "        \n",
    "        self.tt_model = pn.widgets.Select(name='Select', options=['Random Forrest Classifier'])\n",
    "        \n",
    "        \n",
    "    def train_test(self, event):\n",
    "        \n",
    "        #get values from sentiment.\n",
    "        self.display_df = convert_sentiment_values(self.display_df)\n",
    "        \n",
    "        y = self.display_df['label']\n",
    "        \n",
    "        #get train test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, y, test_size = self.test_slider.value/100, random_state = 0)\n",
    "        \n",
    "        \n",
    "        if self.tt_model.value == 'Random Forrest Classifier':\n",
    "            sentiment_classifier = RandomForestClassifier(n_estimators = 1000, random_state = 0)\n",
    "            \n",
    "            sentiment_classifier.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = sentiment_classifier.predict(X_test)\n",
    "            \n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        self.analyze()\n",
    "        \n",
    "    def analyze(self):\n",
    "        self.cm = confusion_matrix(self.y_test,self.y_pred)\n",
    "        self.cr = classification_report(self.y_test,self.y_pred)\n",
    "        self.acc_score = accuracy_score(self.y_test, self.y_pred)\n",
    "        \n",
    "        splits = self.cr.split('\\n')\n",
    "        cml = self.cm.tolist()\n",
    "        self.result_string = f\"\"\"\n",
    "            ### Classification Report\n",
    "            <pre>\n",
    "            {splits[0]}\n",
    "            {splits[1]}\n",
    "            {splits[2]}\n",
    "            {splits[3]}\n",
    "            {splits[4]}\n",
    "            {splits[5]}\n",
    "            {splits[6]}\n",
    "            {splits[7]}\n",
    "            {splits[8]}\n",
    "            </pre>\n",
    "            ### Confusion Matrix\n",
    "            <pre>\n",
    "            {cml[0]}\n",
    "            {cml[1]}\n",
    "\n",
    "            </pre>\n",
    "\n",
    "            ### Accuracy Score\n",
    "            <pre>\n",
    "            {round(self.acc_score, 4)}\n",
    "            </pre\n",
    "            \"\"\"\n",
    "        \n",
    "\n",
    "        self.results = True \n",
    "\n",
    "    def options_page(self, help_text):\n",
    "        \n",
    "        return pn.WidgetBox(help_text, self.tt_model,\n",
    "                            self.test_slider,\n",
    "                            self.tt_button,\n",
    "                height = 375,\n",
    "                width = 300\n",
    "        \n",
    "        )\n",
    "        \n",
    "    @pn.depends('results')\n",
    "    def df_pane(self):\n",
    "        \n",
    "        if self.results == False:\n",
    "            self.result_pane = self.display_df\n",
    "            \n",
    "        else:\n",
    "            self.result_pane = pn.pane.Markdown(f\"\"\"\n",
    "                {self.result_string}\n",
    "                \"\"\", width = 500, height = 350)\n",
    "        \n",
    "        return pn.WidgetBox(self.result_pane,\n",
    "                           height = 375,\n",
    "                           width = 450)\n",
    "        \n",
    "\n",
    "\n",
    "    def panel(self):\n",
    "        \n",
    "        help_text = (\n",
    "            \"Your text will now be trained and tested using a selected model.  You may \" +\n",
    "            \"choose a percentage of your data to reserve for testing, the rest will be used for \" +\n",
    "            \"training.  For example, if I reserve 20%, the rest of the 80% will be used for training \" +\n",
    "            \"and the 20% will be used to determine how well the trained model does assigning a \" +\n",
    "            \"sentiment label to the testing text.  Currently, the only model available is the sklearn \" +\n",
    "            \"Random Forrest Classifier model.\"\n",
    "        )\n",
    "        \n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Train and Test'),\n",
    "                    self.options_page(help_text),\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    self.df_pane,\n",
    "                    \n",
    "                )\n",
    "        \n",
    "        )\n",
    "    \n",
    "    \n",
    "def convert_sentiment_values(df, col = 'sentiment'):\n",
    "    vals = df['sentiment'].unique()\n",
    "    df['label'] = 0\n",
    "\n",
    "    for n in range(len(vals)):\n",
    "        df['label'] = [n if df[col][x] == vals[n] else df['label'][x] for x in range(len(df[col]))]\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to notice about this stage is that it has a couple of parameters common with stage one, `X` and `display_df`.  The second stage needs the result of stage one, `X`, and a little bit of information stored in the `display_df` dataframe in order to be able to do the training and testing.  The `output` function of the first class can now be updated to return the outputs that we need to pass to stage 2. \n",
    "\n",
    "The following will be added to the stage 1 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@param.output('X', 'display_df')\n",
    "def output(self):\n",
    "    return self.X, self.display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a panel pipeline, if you need to pass information from one stage to the next, you need to specify the output on one stage, and have the same parameters specified on the next stage.  The paramters on the second stage will essentially consume the output from the stage before.  In our case, we have already set up the training page to have the same parameters specified.  \n",
    "\n",
    "The final piece needed in place in this panel app is a ready parameter.  This will signal to the pipeline process that we are ready to move from the current stage to the next.  I have chosen to keep it simple and will include a param boolean called `ready` in the PreProcessor class.  The user will click `continue`, the pre-processing will be completed, and then ready will be set to True, which will signal its time to move to stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "\n",
    "from nltk.stem import (PorterStemmer, SnowballStemmer)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class PreProcessor(param.Parameterized):\n",
    "    \n",
    "    # df will be the variable holding the dataframe of text\n",
    "    df = param.DataFrame()\n",
    "    # title to display for each tab\n",
    "    name_of_page = param.String(default = 'Name of page')\n",
    "    # dataframe to display.\n",
    "    display_df = param.DataFrame(default = pd.DataFrame())\n",
    "    # stopword_df is the dataframe containing the stopewords\n",
    "    stopword_df = param.DataFrame(default = pd.DataFrame())\n",
    "    \n",
    "    stopwords = param.List(default = [])\n",
    "    X = param.Array(default = None)\n",
    "    \n",
    "    # *****NEW***********\n",
    "    ready = param.Boolean(\n",
    "        default=False,\n",
    "        doc='trigger for moving to the next page',\n",
    "        )   \n",
    "    # *******************\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # button for the pre-processing page\n",
    "        self.continue_button = pn.widgets.Button(name='Continue',\n",
    "                                                 width = 100,\n",
    "                                                 button_type='primary')\n",
    "\n",
    "        self.continue_button.on_click(self.continue_ready)\n",
    "        \n",
    "        # load text widgets \n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        self.load_file = pn.widgets.FileInput()\n",
    "        self.load_file.link(self.df, callbacks={'value': self.load_df})\n",
    "        self.header_checkbox = pn.widgets.Checkbox(name='Header included in file')\n",
    "        \n",
    "        # tokenize widgets\n",
    "        self.search_pattern_input = pn.widgets.TextInput(name='Search Pattern', value = '\\w+', width = 100)\n",
    "        \n",
    "        # remove stop words widgets\n",
    "        self.load_words_button = pn.widgets.FileInput()\n",
    "        self.load_words_button.link(self.stopwords, callbacks={'value': self.load_stopwords})\n",
    "        \n",
    "        # stem widgets\n",
    "        self.stem_choice = pn.widgets.Select(name='Select', options=['Porter', 'Snowball'])\n",
    "        \n",
    "        # embedding widgets\n",
    "        \n",
    "        self.we_model = pn.widgets.Select(name='Select', options=['SKLearn Count Vectorizer'])\n",
    "\n",
    "        \n",
    "    @param.output('X', 'display_df')\n",
    "    def output(self):\n",
    "        return self.X, self.display_df\n",
    "    \n",
    "    \n",
    "    @param.depends('display_df')\n",
    "    def df_pane(self):\n",
    "        return pn.WidgetBox(self.display_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "    \n",
    "    # load text page functions\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def load_df(self, df, event):\n",
    "        info = io.BytesIO(self.load_file.value)\n",
    "        if self.header_checkbox.value==True:\n",
    "            self.df = pd.read_csv(info)\n",
    "        else:\n",
    "            self.df = pd.read_csv(info, sep='\\n', header = None, names=['text'])\n",
    "        \n",
    "        self.display_df = self.df\n",
    "    \n",
    "    def load_text_page(self):\n",
    "        helper_text = (\n",
    "            \"This simple Sentiment Analysis NLP app will allow you to select a few different options \" +\n",
    "            \"for some preprocessing steps to prepare your text for testing and training. \" +\n",
    "            \"It will then allow you to choose a model to train, the percentage of data to \" +\n",
    "            \"preserve for test, while the rest will be used to train the model.  Finally, \" +\n",
    "            \"some initial metrics will be displayed to determine how well the model did to predict \" +\n",
    "            \"the testing results.\" +\n",
    "            \" \" +\n",
    "            \"Please choose a csv file that contains lines of text to analyze.  This text should \" +\n",
    "            \"have a text column as well as a sentiment column.  If there is a header included in the file, \" +\n",
    "            \"make sure to check the header checkbox.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Text:'),\n",
    "                    pn.Column(\n",
    "                        helper_text,\n",
    "                         self.header_checkbox,\n",
    "                         self.load_file\n",
    "                        ),\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    self.df_pane,\n",
    "                    \n",
    "                )\n",
    "        \n",
    "        )\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # tokenize page options\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    def tokenize_option_page(self):\n",
    "        \n",
    "        help_text = (\"Tokenization will break your text into a list of single articles \" +\n",
    "            \"(ex. ['A', 'cat', 'walked', 'into', 'the', 'house', '.']).  Specify a regular \" +\n",
    "            \"expression (regex) search pattern to use for splitting the text.\")\n",
    "        \n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Tokenize options:'),\n",
    "                    pn.WidgetBox(help_text, self.search_pattern_input,\n",
    "                                    height = 300,\n",
    "                                    width = 300\n",
    "        \n",
    "                                )\n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # remove stopwords page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def remove_stopwords_page(self):\n",
    "        \n",
    "        help_text = (\n",
    "            \"Stop words are words that do not add any value to the sentiment of the text. \" +\n",
    "            \"Removing them may improve your sentiment results.  You may load a list of stop words \" +\n",
    "            \"to exclude from your text.\"\n",
    "        )\n",
    "        return pn.Row(\n",
    "                pn.Column(\n",
    "                    pn.pane.Markdown(f'##Load Stopwords:'),\n",
    "                    pn.WidgetBox(help_text, self.load_words_button,\n",
    "                                    height = 300,\n",
    "                                    width = 300\n",
    "        \n",
    "                    )\n",
    "                ),\n",
    "                pn.Column(\n",
    "                    pn.Spacer(height=52),\n",
    "                    pn.WidgetBox(self.stopword_df,\n",
    "                           height = 300,\n",
    "                           width = 400)\n",
    "                    \n",
    "                )\n",
    "        )\n",
    "    \n",
    "    def load_stopwords(self, stopwords, event):\n",
    "        info = io.BytesIO(self.load_words_button.value)\n",
    "        self.stopwords = pd.read_pickle(info)\n",
    "        self.stopword_df = pd.DataFrame({'stop words': self.stopwords})\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # stemming page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def stemmer_page(self):\n",
    "        help_text = (\n",
    "            \"Stemming is a normalization step for the words in your text.  Something that is \" +\n",
    "            \"plural should probably still be clumped together with a singular version of a word, \" +\n",
    "            \"for example.  Stemming will basically remove the ends of words.  Here you can choose \" + \n",
    "            \"between a Porter Stemmer or Snowball Stemmer. Porter is a little less aggressive than \" +\n",
    "            \"Snowball, however, Snowball is considered a slight improvement over Porter.\"\n",
    "        )\n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Stemmer options:'),\n",
    "                    pn.WidgetBox(help_text, self.stem_choice,\n",
    "                height = 300,\n",
    "                width = 300)\n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # embedding page \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def word_embedding_page(self):\n",
    "        \n",
    "        help_text = (\"Embedding the process of turning words into numerical vectors. \" +\n",
    "                    \"There have been several algorithms developed to do this, however, currently in this \" +\n",
    "                    \"app, the sklearn count vectorizer is available. This algorithm will return a sparse \" +\n",
    "                    \"matrix represention of all the words in your text.\"\n",
    "                    )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return pn.Column(\n",
    "                    pn.pane.Markdown(f'##Choose embedding model:'),\n",
    "                    pn.WidgetBox(help_text, self.we_model,\n",
    "                            height = 300,\n",
    "                            width = 300\n",
    "        \n",
    "                    )\n",
    "        \n",
    "                )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "          \n",
    "    def continue_ready(self, event):\n",
    "\n",
    "        # Set up for tokenization\n",
    "        tokenizer = RegexpTokenizer(self.search_pattern_input.value)\n",
    "\n",
    "        # Set up for stemming\n",
    "        if self.stem_choice.value == 'Porter':\n",
    "            stemmer = PorterStemmer() \n",
    "        else:\n",
    "            stemmer = SnowballStemmer()\n",
    "\n",
    "        # Set up for embedding\n",
    "        if self.we_model.value == 'SKLearn Count Vectorizer':\n",
    "            # Create a vectorizer instance\n",
    "            vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "        corpus = []\n",
    "        #loop through each line of data\n",
    "        for n in range(len(self.display_df)):  \n",
    "            sentence = self.display_df.iloc[n].text\n",
    "\n",
    "            #1. Tokenize\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "            #2. remove stop words\n",
    "            tokens_no_sw = [word for word in tokens if not word in self.stopwords]\n",
    "\n",
    "            #3. stem the remaining words\n",
    "            stem_words = [stemmer.stem(x) for x in tokens_no_sw]\n",
    "\n",
    "            #Join the words back together as one string and append this string to your corpus.\n",
    "            corpus.append(' '.join(stem_words))\n",
    "\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        labels = self.display_df['sentiment']\n",
    "\n",
    "        xlist = []\n",
    "        for n in range(len(X)):\n",
    "            xlist.append(list(X[n]))\n",
    "        self.X = X\n",
    "        self.display_df = pd.DataFrame({'embeddings': xlist, 'sentiment': labels})\n",
    "        \n",
    "        self.ready = True\n",
    "    \n",
    "    def panel(self):\n",
    "        \n",
    "        return pn.Column(\n",
    "            pn.Tabs(\n",
    "                ('Load Text', self.load_text_page),\n",
    "                ('Tokenize', self.tokenize_option_page),\n",
    "                ('Remove Stopwords', self.remove_stopwords_page),\n",
    "                ('Stem', self.stemmer_page),\n",
    "                ('Embed', self.word_embedding_page)\n",
    "                ),\n",
    "            self.continue_button\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to put our panel pipeline code into place.  For details on how to complete the pipeline, please refer back to this post (reference to pipeline post). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
